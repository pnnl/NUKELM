{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "This material was prepared as an account of work sponsored by an agency of the United States Government.  Neither the United States Government nor the United States Department of Energy, nor Battelle, nor any of their employees, nor any jurisdiction or organization that has cooperated in the development of these materials, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness or any information, apparatus, product, software, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof, or Battelle Memorial Institute. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.\n",
    "\n",
    "PACIFIC NORTHWEST NATIONAL LABORATORY operated by BATTELLE for the UNITED STATES DEPARTMENT OF ENERGY under Contract DE-AC05-76RL01830."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune NukeLM on Azure Databricks\n",
    "\n",
    "Wraps `nukelm.run_fine_tune` to install dependencies, provide a drop-down menu for parameters, and copy results to Azure storage. It utilizes multiple GPUs and integrates with MLFlow.\n",
    "\n",
    "Please use Databricks Runtime 7.3 LTS ML (preferably with GPU support).\n",
    "\n",
    "| Parameter | Description |\n",
    "| --- | --- |\n",
    "| Model Name | Short-hand name for a pre-trained language model starting point. |\n",
    "| Label Type | Whether to use fine-grained (multi-class) or coarse-grained (binary) labels. |\n",
    "| RNG Seed | Integer with which to seed a random number generator. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this approach is for running on DataBricks and assumes installation of nukelm via the wheel file.\n",
    "%pip install nukelm-1.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "54de8396-4ed8-4e7c-b78f-95b85610a762",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from nukelm import run_fine_tune\n",
    "\n",
    "\n",
    "# disable noisy py4j logger\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)\n",
    "\n",
    "# locations on Azure storage\n",
    "blob_path = Path(\"\")\n",
    "assert blob_path.exists()\n",
    "\n",
    "working_path = blob_path\n",
    "dbfs_working_path = blob_path\n",
    "\n",
    "src_path = working_path / \"src\"\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "data_path = working_path / \"osti\" / \"finetune\"\n",
    "DATASET_SIZE = 188654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9824bc8b-0998-44e0-bf87-a3544c6f7098",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"roberta_base-ots\": {\n",
    "        \"Model Name or Path\": \"roberta-base\",\n",
    "        \"Tokenizer Name\": \"roberta-base\",\n",
    "    },\n",
    "    \"roberta_base-trained\": {\n",
    "        \"Model Name or Path\": \"/databricks/mlflow/1636808823342661/b61deb3377ac412291956719bf0ca952/artifacts/model\",\n",
    "        \"Tokenizer Name\": \"roberta-base\",\n",
    "    },\n",
    "    \"roberta_large-ots\": {\n",
    "        \"Model Name or Path\": \"roberta-large\",\n",
    "        \"Tokenizer Name\": \"roberta-large\",\n",
    "    },\n",
    "    \"roberta_large-trained\": {\n",
    "        \"Model Name or Path\": str(working_path / \"models\" / \"MLM\"),\n",
    "        \"Tokenizer Name\": \"roberta-large\",\n",
    "    },\n",
    "    \"scibert-ots\": {\n",
    "        \"Model Name or Path\": \"allenai/scibert_scivocab_uncased\",\n",
    "        \"Tokenizer Name\": \"allenai/scibert_scivocab_uncased\",\n",
    "    },\n",
    "    \"scibert-trained\": {\n",
    "        \"Model Name or Path\": \"/databricks/mlflow/1636808823342661/fcbe913b86cd4e5c807efad7cf16ef74/artifacts/model\",\n",
    "        \"Tokenizer Name\": \"allenai/scibert_scivocab_uncased\",\n",
    "    },\n",
    "}\n",
    "\n",
    "model_names = list(MODELS.keys())\n",
    "dbutils.widgets.dropdown(\"Model Name\", model_names[0], model_names)  # type: ignore # NOQA: F821\n",
    "dbutils.widgets.dropdown(\"Label Type\", \"Coarse\", [\"Fine\", \"Coarse\"])  # type: ignore # NOQA: F821\n",
    "dbutils.widgets.text(\"RNG Seed\", \"42\")  # type: ignore # NOQA: F821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cc2922f9-d041-405a-af5a-1ad233bfab15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = dbutils.widgets.get(\"Model Name\")  # type: ignore # NOQA: F821\n",
    "label_type = dbutils.widgets.get(\"Label Type\")  # type: ignore # NOQA: F821\n",
    "rng_seed = int(dbutils.widgets.get(\"RNG Seed\"))  # type: ignore # NOQA: F821\n",
    "\n",
    "n_devices = torch.cuda.device_count()\n",
    "\n",
    "run_dir = Path(f\"/tmp/run_{model_name.replace('_', '-')}_{label_type.lower()}-grained-labels_{rng_seed}\")\n",
    "try:\n",
    "    run_dir.mkdir()\n",
    "except FileExistsError:\n",
    "    print(\"We've run with that seed before -- try a new one?\")\n",
    "\n",
    "params = {\n",
    "    \"model_name_or_path\": MODELS[model_name][\"Model Name or Path\"],\n",
    "    \"tokenizer_name\": MODELS[model_name][\"Tokenizer Name\"],\n",
    "    \"train_file\": str(data_path / f\"{label_type.lower()}-grained-labels\" / \"train.csv\"),\n",
    "    \"validation_file\": str(data_path / f\"{label_type.lower()}-grained-labels\" / \"val.csv\"),\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"output_dir\": str(run_dir / \"output\"),\n",
    "    \"logging_dir\": str(run_dir / \"logs\"),\n",
    "    \"seed\": rng_seed,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"fp16\": True,\n",
    "    \"n_gpu\": n_devices,\n",
    "}\n",
    "\n",
    "effective_batch_size = (\n",
    "    params[\"per_device_train_batch_size\"] * params[\"gradient_accumulation_steps\"] * torch.cuda.device_count()\n",
    ")\n",
    "n_steps = params[\"num_train_epochs\"] * np.ceil(DATASET_SIZE / effective_batch_size)\n",
    "\n",
    "params[\"warmup_steps\"] = int(0.06 * n_steps)\n",
    "\n",
    "logging_steps = int(np.floor(n_steps / 20))\n",
    "params[\"logging_steps\"] = logging_steps\n",
    "params[\"save_steps\"] = logging_steps\n",
    "params[\"eval_steps\"] = logging_steps\n",
    "\n",
    "with open(run_dir / \"training_config.json\", \"w\") as fh:\n",
    "    json.dump(params, fh)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "630e2472-5c72-4a85-aa9c-73d964bf0f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_fine_tune(config_path=str(run_dir / \"training_config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bc062043-b5dc-4b32-8a04-4eebd4970e72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_dir = (\n",
    "    working_path\n",
    "    / \"finetune_output\"\n",
    "    / f\"run_{model_name.replace('_', '-')}_{label_type.lower()}-grained-labels_{rng_seed}\"\n",
    ")\n",
    "save_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2d6b0e48-4724-41c2-8743-f9ad1c3b5452",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files_to_save = [\n",
    "    \"config.json\",\n",
    "    \"merges.txt\",\n",
    "    \"pytorch_model.bin\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"training_args.bin\",\n",
    "    \"vocab.txt\",\n",
    "]\n",
    "\n",
    "shutil.copy2(run_dir / \"config.json\", save_dir / \"training-config.json\")\n",
    "for f in files_to_save:\n",
    "    filepath = run_dir / \"output\" / f\n",
    "    if not (filepath).exists():\n",
    "        print(f\"File {filepath} does not exist\")\n",
    "        continue\n",
    "    shutil.copy2(filepath, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "46c9fa5a-c94b-4e7f-bc5e-be5a9728b18b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "fine-tune",
   "notebookOrigID": 360535408413965,
   "widgets": {
    "Label Type": {
     "currentValue": "Coarse",
     "nuid": "45fe306a-519b-4766-9e58-7032dccae491",
     "widgetInfo": {
      "defaultValue": "Coarse",
      "label": null,
      "name": "Label Type",
      "options": {
       "choices": [
        "Fine",
        "Coarse"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "Model Name": {
     "currentValue": "roberta_large-trained",
     "nuid": "340cba65-ea1a-4603-b573-8e211fe974ca",
     "widgetInfo": {
      "defaultValue": "roberta_base-ots",
      "label": null,
      "name": "Model Name",
      "options": {
       "choices": [
        "roberta_base-ots",
        "roberta_base-trained",
        "roberta_large-ots",
        "roberta_large-trained",
        "scibert-ots",
        "scibert-trained"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "RNG Seed": {
     "currentValue": "0",
     "nuid": "c1833be5-8b1f-48df-80ca-98e9faa0278f",
     "widgetInfo": {
      "defaultValue": "42",
      "label": null,
      "name": "RNG Seed",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
