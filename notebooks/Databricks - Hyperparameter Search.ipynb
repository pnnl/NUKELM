{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "This material was prepared as an account of work sponsored by an agency of the United States Government.  Neither the United States Government nor the United States Department of Energy, nor Battelle, nor any of their employees, nor any jurisdiction or organization that has cooperated in the development of these materials, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness or any information, apparatus, product, software, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof, or Battelle Memorial Institute. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.\n",
    "\n",
    "PACIFIC NORTHWEST NATIONAL LABORATORY operated by BATTELLE for the UNITED STATES DEPARTMENT OF ENERGY under Contract DE-AC05-76RL01830."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search for Fine-Tuning on Azure Databricks\n",
    "\n",
    "Wraps `nukelm.run_fine_tune` to install dependencies, provide a drop-down menu for parameters, and copy results to Azure storage. It utilizes multiple GPUs and integrates with MLFlow.\n",
    "\n",
    "The search space is assumed to be a simple grid to be explored exhaustively. `hyperopt` is used for its Spark integrations, not its adpative search algorithms.\n",
    "\n",
    "Please use Databricks Runtime 7.3 LTS ML (preferably with GPU support).\n",
    "\n",
    "| Parameter | Description |\n",
    "| --- | --- |\n",
    "| Label Type | Whether to use fine-grained (multi-class) or coarse-grained (binary) labels. |\n",
    "| RNG Seed | Integer with which to seed a random number generator. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this approach is for running on DataBricks and assumes installation of nukelm via the wheel file.\n",
    "%pip install nukelm-1.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b238ab95-30e5-491d-ae26-e4a44e55b9c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import uuid\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import hyperopt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from nukelm import run_fine_tune\n",
    "from nukelm.hyperopt_utils import suggest, validate_space_exhaustive_search\n",
    "\n",
    "\n",
    "# disable noisy py4j logger\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)\n",
    "\n",
    "# locations on Azure storage\n",
    "blob_path = Path(\"\")\n",
    "assert blob_path.exists()\n",
    "\n",
    "working_path = blob_path \n",
    "dbfs_working_path = blob_path \n",
    "\n",
    "data_path = working_path / \"osti\" / \"finetune\"\n",
    "DATASET_SIZE = 188654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2dd3a293-725a-413d-a7f1-c020624fcb17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "space = {\n",
    "    \"Batch Size\": hyperopt.hp.choice(\"batch_size\", [16, 64]),\n",
    "    \"Learning Rate\": hyperopt.hp.choice(\"learning_rate\", [1e-5, 2e-5, 5e-5]),\n",
    "    \"Model Name\": hyperopt.hp.choice(\n",
    "        \"Model Name\",\n",
    "        [\n",
    "            \"roberta_base-ots\",\n",
    "            \"roberta_base-trained\",\n",
    "            \"roberta_large-ots\",\n",
    "            \"roberta_large-trained\",\n",
    "            \"scibert-ots\",\n",
    "            \"scibert-trained\",\n",
    "        ],\n",
    "    ),\n",
    "}\n",
    "# raises error if exhaustive search not possible\n",
    "validate_space_exhaustive_search(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of iterations by running through the search space once\n",
    "_ALL_ARGS = []\n",
    "\n",
    "\n",
    "def dummy_objective(args):\n",
    "    \"\"\"Make list of all args from exhaustive search.\"\"\"\n",
    "    _ALL_ARGS.append(args)\n",
    "    return 0\n",
    "\n",
    "\n",
    "_ = hyperopt.fmin(\n",
    "    fn=dummy_objective,\n",
    "    space=space,\n",
    "    trials=hyperopt.Trials(),\n",
    "    algo=partial(suggest, nbMaxSucessiveFailures=1000),\n",
    "    max_evals=np.inf,\n",
    "    show_progressbar=False,\n",
    ")\n",
    "NUM_ARGS = len(_ALL_ARGS)\n",
    "\n",
    "print(f\"Running {NUM_ARGS} training iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "add33a83-1937-44dd-bf6d-cc895232efc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"roberta_base-ots\": {\n",
    "        \"Model Name or Path\": \"roberta-base\",\n",
    "        \"Tokenizer Name\": \"roberta-base\",\n",
    "    },\n",
    "    \"roberta_base-trained\": {\n",
    "        \"Model Name or Path\": str(working_path / \"tmp_pretraining_input\" / \"roberta-base\"),\n",
    "        \"Tokenizer Name\": \"roberta-base\",\n",
    "    },\n",
    "    \"roberta_large-ots\": {\n",
    "        \"Model Name or Path\": \"roberta-large\",\n",
    "        \"Tokenizer Name\": \"roberta-large\",\n",
    "    },\n",
    "    \"roberta_large-trained\": {\n",
    "        \"Model Name or Path\": str(working_path / \"tmp_pretraining_input\" / \"roberta-large\"),\n",
    "        \"Tokenizer Name\": \"roberta-large\",\n",
    "    },\n",
    "    \"scibert-ots\": {\n",
    "        \"Model Name or Path\": \"allenai/scibert_scivocab_uncased\",\n",
    "        \"Tokenizer Name\": \"allenai/scibert_scivocab_uncased\",\n",
    "    },\n",
    "    \"scibert-trained\": {\n",
    "        \"Model Name or Path\": str(working_path / \"tmp_pretraining_input\" / \"scibert\"),\n",
    "        \"Tokenizer Name\": \"allenai/scibert_scivocab_uncased\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"Label Type\", \"Coarse\", [\"Fine\", \"Coarse\"])  # type: ignore # NOQA: F821\n",
    "dbutils.widgets.text(\"RNG Seed\", \"42\")  # type: ignore # NOQA: F821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "89ab33e6-033e-47f1-afac-2c1e26a683c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LABEL_TYPE = dbutils.widgets.get(\"Label Type\")  # type: ignore # NOQA: F821\n",
    "rng_seed = int(dbutils.widgets.get(\"RNG Seed\"))  # type: ignore # NOQA: F821\n",
    "\n",
    "n_devices = torch.cuda.device_count()\n",
    "\n",
    "FIXED_PARAMS = {\n",
    "    \"train_file\": str(data_path / f\"{LABEL_TYPE.lower()}-grained-labels\" / \"train.txt\"),\n",
    "    \"validation_file\": str(data_path / f\"{LABEL_TYPE.lower()}-grained-labels\" / \"val.txt\"),\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"seed\": rng_seed,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"fp16\": True,\n",
    "    \"n_gpu\": n_devices,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "882e305f-0248-44e5-845b-0fa9a747a7a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train(params: Dict):\n",
    "    \"\"\"Perform single fine-tuning run.\n",
    "\n",
    "    Args:\n",
    "        params (Dict): Training parameters which change run-to-run.\n",
    "\n",
    "    Returns:\n",
    "        Dict: `hyperopt` results including status and loss, if successful.\n",
    "    \"\"\"\n",
    "    # avoid collisions with other runs\n",
    "    run_uuid = uuid.uuid4()\n",
    "    print(f\"Run UUID: {run_uuid}\")\n",
    "\n",
    "    # cloud storage for intermediate outputs (to accomdate arbitrary number of serialized models)\n",
    "    output_path = working_path / \"tmp_training_output\" / f\"{run_uuid}\"\n",
    "    output_path.mkdir()\n",
    "    params[\"output_dir\"] = str(output_path)\n",
    "\n",
    "    # tensorboard does not connect properly to mounted cloud storage\n",
    "    log_path = Path(\"/tmp\") / \"logs\" / f\"{run_uuid}\"\n",
    "    log_path.mkdir(parents=True)\n",
    "    config_path = log_path / \"training_config.json\"\n",
    "    params[\"logging_dir\"] = str(log_path)\n",
    "\n",
    "    # override defaults for some parameters\n",
    "    params.update(FIXED_PARAMS)\n",
    "\n",
    "    model_name = params.pop(\"Model Name\")\n",
    "    params[\"model_name_or_path\"] = MODELS[model_name][\"Model Name or Path\"]\n",
    "    params[\"tokenizer_name\"] = MODELS[model_name][\"Tokenizer Name\"]\n",
    "\n",
    "    params[\"learning_rate\"] = params.pop(\"Learning Rate\")\n",
    "\n",
    "    # calculate gradient accumulation steps necessary to achieve effective minibatch size\n",
    "    batch_size = params.pop(\"Batch Size\")\n",
    "    n_devices = torch.cuda.device_count()\n",
    "    gradient_accumulation_steps = batch_size / params[\"per_device_train_batch_size\"] / n_devices\n",
    "    assert params[\"gradient_accumulation_steps\"] % 1 == 0, (\n",
    "        f\"Check interplay of batch size ({batch_size}), gradient accumulation \"\n",
    "        f\"({gradient_accumulation_steps}), and multiple GPUs ({n_devices})\"\n",
    "    )\n",
    "    params[\"gradient_accumulation_steps\"] = gradient_accumulation_steps\n",
    "\n",
    "    n_samples = DATASET_SIZE  # samples without resampling\n",
    "    n_steps = int(params[\"num_train_epochs\"] * np.ceil(n_samples / batch_size))\n",
    "\n",
    "    # adjust warmup steps to batch size\n",
    "    params[\"warmup_steps\"] = int(0.06 * n_steps)\n",
    "\n",
    "    # evaluate & save ~20 times in training, regardless of batch size\n",
    "    logging_steps = int(np.floor(n_steps / 20))\n",
    "    params[\"logging_steps\"] = logging_steps\n",
    "    params[\"save_steps\"] = logging_steps\n",
    "    params[\"eval_steps\"] = logging_steps\n",
    "\n",
    "    with open(config_path, \"w\") as handle:\n",
    "        json.dump(params, handle)\n",
    "\n",
    "    # attempt to clear GPU cache from any previous runs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    try:\n",
    "        metrics = run_fine_tune(config_path)\n",
    "    except Exception as ex:\n",
    "        print(repr(ex))\n",
    "        return {\"status\": hyperopt.STATUS_FAIL}\n",
    "\n",
    "    # backup Tensorboard logs\n",
    "    shutil.copytree(log_path, Path(output_path) / \"raw_logs\")\n",
    "\n",
    "    result = {\n",
    "        \"loss\": metrics[\"eval_loss\"],\n",
    "        \"status\": hyperopt.STATUS_OK,\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9130386c-066e-4c72-a26e-a4c8ac15679d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_trials = True  # Set to False to run on head node only\n",
    "parallelism = 2  # Set to number of nodes in cluster\n",
    "evals_to_run = NUM_ARGS\n",
    "\n",
    "# SparkTrials do not seem to be picklable for re-starting later\n",
    "if spark_trials:\n",
    "    trials = hyperopt.SparkTrials(parallelism=parallelism)\n",
    "    rstate = np.random.RandomState(42)\n",
    "else:\n",
    "    trials_file = os.path.join(working_path, \"tmp_training_output\", f\"hyperopt-trials_{LABEL_TYPE.lower()}.pkl\")\n",
    "    try:\n",
    "        with open(trials_file, \"rb\") as handle:\n",
    "            trials, rstate = pickle.load(handle)\n",
    "        print(\"Loaded saved Trials\")\n",
    "    except Exception:\n",
    "        print(\"Starting a new Trials object\")\n",
    "        trials = hyperopt.Trials()\n",
    "        rstate = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d8c74cd0-a162-4a35-8906-b225841e38fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# default is 4 -- retries unlikely to fix errors\n",
    "spark.conf.set(\"spark.task.maxFailures\", 1)  # type: ignore # NOQA: F821\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run():\n",
    "        best = hyperopt.fmin(\n",
    "            train,\n",
    "            space=space,\n",
    "            algo=suggest,\n",
    "            max_evals=evals_to_run,\n",
    "            trials=trials,\n",
    "            rstate=rstate,\n",
    "        )\n",
    "finally:\n",
    "    mlflow.end_run()\n",
    "    if not spark_trials:\n",
    "        with open(trials_file, \"wb\") as handle:\n",
    "            pickle.dump((trials, rstate), handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "de65fd17-5d31-4c99-947a-0dfb83acff42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "Binary Large Hyperparameter Search",
   "notebookOrigID": 1636808823342689,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
